{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOcLT9XzRSGUdsBm8SUy984",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bindulekh/private/blob/main/NLP_exercise1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U-OfzMpJFfhf"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet # Fill in the blank\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer # Fill in the blank\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize# Fill in the blank\n",
        "import re\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUoGbtfJGasb",
        "outputId": "8ae35694-4641-4503-bea1-aa94ac2d444c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text\n",
        "text = \"\"\"\n",
        "Natural language processing (NLP) is a subfield of linguistics, computer\n",
        "science, and artificial intelligence.\n",
        "It involves the interactions between computers and humans using the natural\n",
        "language. The ultimate objective\n",
        "of NLP is to read, decipher, understand, and make sense of the human language in\n",
        "a valuable way. It started\n",
        "in the 1950s, although work can be found from earlier periods. In 1950, Alan\n",
        "Turing published an article titled\n",
        "\"Computing Machinery and Intelligence\" which proposed what is now called the\n",
        "Turing test as a criterion of\n",
        "intelligence, a task that involves the automated interpretation and generation\n",
        "of natural language, but at the\n",
        "time not articulated as a problem separate from artificial intelligence. The\n",
        "premise of symbolic NLP is\n",
        "well-summarized by John Searle's Chinese room experiment: Given a collection of\n",
        "rules (e.g., a Chinese phrasebook,\n",
        "with questions and matching answers), the computer emulates natural language\n",
        "understanding (or other NLP tasks)\n",
        "by applying those rules to the data it is confronted with. 2023 is the year when\n",
        "NLP got its major breakthrough.\n",
        "\"\"\"\n",
        "# Task 1: Tokenization\n",
        "# Write a function to tokenize the text and return the tokens for first line inthe paragraph\n",
        "def tokenize_text(text):\n",
        " sentences = sent_tokenize(text)\n",
        "         # Get the first sentence\n",
        " first_sentence = sentences[0]\n",
        "        # Tokenize the first sentence\n",
        " tokens = word_tokenize(first_sentence)\n",
        " return tokens\n",
        "\n"
      ],
      "metadata": {
        "id": "MZ4l8y8NGeAR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenize_text(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVAudu4CKsP9",
        "outputId": "35057c63-7e30-4ad5-e8e9-65e2ba61ebf5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Stop Word Removal\n",
        "#Write a function to remove stop words for whole paragraph from the tokens and return the filtered tokens\n",
        "from nltk.corpus import stopwords\n",
        "def remove_stop_words(text):\n",
        "    # Tokenize the paragraph into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Combine all sentences into a single string\n",
        "    combined_text = ' '.join(sentences)\n",
        "\n",
        "    # Tokenize the combined text into words\n",
        "    tokens = word_tokenize(combined_text)\n",
        "\n",
        "    # Get the list of English stop words\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "    # Filter out stop words from the tokens\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "filtered_tokens = remove_stop_words(text)\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE4RcaHkLgVU",
        "outputId": "5184652b-58d5-4960-c2a1-c62218f4cf66"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', '.', 'involves', 'interactions', 'computers', 'humans', 'using', 'natural', 'language', '.', 'ultimate', 'objective', 'NLP', 'read', ',', 'decipher', ',', 'understand', ',', 'make', 'sense', 'human', 'language', 'valuable', 'way', '.', 'started', '1950s', ',', 'although', 'work', 'found', 'earlier', 'periods', '.', '1950', ',', 'Alan', 'Turing', 'published', 'article', 'titled', \"''\", 'Computing', 'Machinery', 'Intelligence', \"''\", 'proposed', 'called', 'Turing', 'test', 'criterion', 'intelligence', ',', 'task', 'involves', 'automated', 'interpretation', 'generation', 'natural', 'language', ',', 'time', 'articulated', 'problem', 'separate', 'artificial', 'intelligence', '.', 'premise', 'symbolic', 'NLP', 'well-summarized', 'John', 'Searle', \"'s\", 'Chinese', 'room', 'experiment', ':', 'Given', 'collection', 'rules', '(', 'e.g.', ',', 'Chinese', 'phrasebook', ',', 'questions', 'matching', 'answers', ')', ',', 'computer', 'emulates', 'natural', 'language', 'understanding', '(', 'NLP', 'tasks', ')', 'applying', 'rules', 'data', 'confronted', '.', '2023', 'year', 'NLP', 'got', 'major', 'breakthrough', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Stemming\n",
        "# Write a function to perform stemming on the filtered tokens and return the stemmed tokens\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def perform_stemming(filtered_tokens):\n",
        "    # Initialize the Porter Stemmer\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Apply stemming to each token\n",
        "    stemmed_tokens = [stemmer.stem(filtered_tokens) for filtered_tokens in filtered_tokens]\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "stemmed_tokens = perform_stemming(filtered_tokens)\n",
        "print(stemmed_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7gVTcCtLgJJ",
        "outputId": "05c8d292-3e79-432d-bb9c-c5f41ab78146"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur', 'languag', 'process', '(', 'nlp', ')', 'subfield', 'linguist', ',', 'comput', 'scienc', ',', 'artifici', 'intellig', '.', 'involv', 'interact', 'comput', 'human', 'use', 'natur', 'languag', '.', 'ultim', 'object', 'nlp', 'read', ',', 'deciph', ',', 'understand', ',', 'make', 'sens', 'human', 'languag', 'valuabl', 'way', '.', 'start', '1950', ',', 'although', 'work', 'found', 'earlier', 'period', '.', '1950', ',', 'alan', 'ture', 'publish', 'articl', 'titl', \"''\", 'comput', 'machineri', 'intellig', \"''\", 'propos', 'call', 'ture', 'test', 'criterion', 'intellig', ',', 'task', 'involv', 'autom', 'interpret', 'gener', 'natur', 'languag', ',', 'time', 'articul', 'problem', 'separ', 'artifici', 'intellig', '.', 'premis', 'symbol', 'nlp', 'well-summar', 'john', 'searl', \"'s\", 'chines', 'room', 'experi', ':', 'given', 'collect', 'rule', '(', 'e.g.', ',', 'chines', 'phrasebook', ',', 'question', 'match', 'answer', ')', ',', 'comput', 'emul', 'natur', 'languag', 'understand', '(', 'nlp', 'task', ')', 'appli', 'rule', 'data', 'confront', '.', '2023', 'year', 'nlp', 'got', 'major', 'breakthrough', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Lemmatization\n",
        "# Write a function to perform lemmatization on the filtered tokens and return the lemmatized tokens\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def perform_lemmatization(filtered_tokens):\n",
        "    # Initialize the WordNet Lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Apply lemmatization to each token\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "lemmatized_tokens = perform_lemmatization(filtered_tokens)\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKX-ami-Lf8F",
        "outputId": "0ae52aef-73fe-48c8-e5c0-7fefaf26c2fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', '.', 'involves', 'interaction', 'computer', 'human', 'using', 'natural', 'language', '.', 'ultimate', 'objective', 'NLP', 'read', ',', 'decipher', ',', 'understand', ',', 'make', 'sense', 'human', 'language', 'valuable', 'way', '.', 'started', '1950s', ',', 'although', 'work', 'found', 'earlier', 'period', '.', '1950', ',', 'Alan', 'Turing', 'published', 'article', 'titled', \"''\", 'Computing', 'Machinery', 'Intelligence', \"''\", 'proposed', 'called', 'Turing', 'test', 'criterion', 'intelligence', ',', 'task', 'involves', 'automated', 'interpretation', 'generation', 'natural', 'language', ',', 'time', 'articulated', 'problem', 'separate', 'artificial', 'intelligence', '.', 'premise', 'symbolic', 'NLP', 'well-summarized', 'John', 'Searle', \"'s\", 'Chinese', 'room', 'experiment', ':', 'Given', 'collection', 'rule', '(', 'e.g.', ',', 'Chinese', 'phrasebook', ',', 'question', 'matching', 'answer', ')', ',', 'computer', 'emulates', 'natural', 'language', 'understanding', '(', 'NLP', 'task', ')', 'applying', 'rule', 'data', 'confronted', '.', '2023', 'year', 'NLP', 'got', 'major', 'breakthrough', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "End of code"
      ],
      "metadata": {
        "id": "65_EhBcROhHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cepHLWGwOg7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N_-5pOPrOgvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}