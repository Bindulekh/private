{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bindulekh/private/blob/main/Transformer_date_converter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emg3z8nKttdC"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nm4reV5Mtqmv"
      },
      "outputs": [],
      "source": [
        "\n",
        "def find_last_checkpoint(checkpoint_dir):\n",
        "    epochs = []\n",
        "    for name in os.listdir(checkpoint_dir):\n",
        "        if os.path.splitext(name)[-1] == '.pth':\n",
        "            epochs += [int(name.strip('ckpt_epoch_.pth'))]\n",
        "    if len(epochs) == 0:\n",
        "        raise IOError('no checkpoint found in {}'.format(checkpoint_dir))\n",
        "    return max(epochs)\n",
        "\n",
        "def save_checkpoint(checkpoint_dir, epoch, model, optimizer=None):\n",
        "    checkpoint = {}\n",
        "    checkpoint['epoch'] = epoch\n",
        "\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        model_state_dict = model.module.state_dict()\n",
        "    else:\n",
        "        model_state_dict = model.state_dict()\n",
        "    checkpoint['model'] = model_state_dict\n",
        "\n",
        "    if optimizer is not None:\n",
        "        optimizer_state_dict = optimizer.state_dict()\n",
        "        # for k, v in optimizer_state_dict.items():\n",
        "        #     print(k, type(v))\n",
        "        # optimizer_state_dict = rename_dict_key(optimizer_state_dict)\n",
        "        checkpoint['optimizer'] = optimizer_state_dict\n",
        "    else:\n",
        "        checkpoint['optimizer'] = None\n",
        "\n",
        "    torch.save(checkpoint, os.path.join(checkpoint_dir, 'ckpt_epoch_%02d.pth'% epoch))\n",
        "\n",
        "def load_checkpoint(checkpoint_dir, epoch=-1):\n",
        "    if epoch == -1:\n",
        "        epoch = find_last_checkpoint(checkpoint_dir)\n",
        "    checkpoint_name = 'ckpt_epoch_%02d.pth'% epoch\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
        "    ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
        "    return ckpt\n",
        "\n",
        "def save_model(checkpoint_dir, epoch, model):\n",
        "    save_checkpoint(checkpoint_dir, epoch, model, optimizer=None)\n",
        "\n",
        "def load_model(checkpoint_dir, epoch, model):\n",
        "    try:\n",
        "        ckpt = load_checkpoint(checkpoint_dir, epoch)\n",
        "        model_state_dict = ckpt['model']\n",
        "\n",
        "        if isinstance(model, torch.nn.DataParallel):\n",
        "            model.module.load_state_dict(model_state_dict)\n",
        "        # elif isinstance(model, torchDDP):\n",
        "        #     model.module.load_state_dict(model_state_dict)\n",
        "        # elif isinstance(model, apexDDP):\n",
        "        #     model.module.load_state_dict(model_state_dict)\n",
        "        else:\n",
        "            model.load_state_dict(model_state_dict)\n",
        "    except Exception as e:\n",
        "        print('failed to load model, {}'.format(e))\n",
        "    return model\n",
        "\n",
        "def load_optimizer(checkpoint_dir, epoch, optimizer):\n",
        "    try:\n",
        "        ckpt = load_checkpoint(checkpoint_dir, epoch)\n",
        "        optimizer_state_dict = ckpt['optimizer']\n",
        "        optimizer.load_state_dict(optimizer_state_dict)\n",
        "    except Exception as e:\n",
        "        print('failed to load optimizer, {}'.format(e))\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RgGGJzVtxy_"
      },
      "source": [
        "## model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZZ9Pf-JsUCoO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VKJdAbEft1NO",
        "outputId": "d94fb63b-efc3-48ca-ed69-535614ef03cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "925.55KB\n",
            "torch.Size([16, 7, 12])\n",
            "tensor([0.1349, 0.0813, 0.1177, 0.0615, 0.0318, 0.0300, 0.1234, 0.0351, 0.0859,\n",
            "        0.1598, 0.1010, 0.0376], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class PositionalEncoding(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, n_position=50):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        self.linear = torch.nn.Linear(in_features=in_dim, out_features=out_dim) #word embedding\n",
        "        # Not a parameter\n",
        "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, out_dim))\n",
        "\n",
        "    def _get_sinusoid_encoding_table(self, n_position, out_dim):\n",
        "        ''' Sinusoid position encoding table '''\n",
        "        # TODO: make it with torch instead of numpy\n",
        "\n",
        "        def get_position_angle_vec(position):\n",
        "            return [position / np.power(10000, 2 * (hid_j // 2) / out_dim) for hid_j in range(out_dim)]\n",
        "\n",
        "        sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)])\n",
        "        sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n",
        "        sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n",
        "\n",
        "        return torch.FloatTensor(sinusoid_table).unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        return x + self.pos_table[:, :x.size(1)].clone().detach()\n",
        "\n",
        "class ScaledDotProductAttention(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        d_k = q.shape[-1]\n",
        "\n",
        "        scores = torch.matmul(q / (d_k ** 0.5), k.transpose(2, 3)) #(N, n_head, T, T)\n",
        "        if mask is not None:\n",
        "            # print(mask.unsqueeze(0).unsqueeze(0).shape, scores.shape)\n",
        "            scores = scores.masked_fill(mask.unsqueeze(0).unsqueeze(0)==0, -1e9)\n",
        "        scores = torch.nn.Softmax(dim=-1)(scores) #(N, n_head, T, T)\n",
        "        # print(scores.shape, scores[2, 1, 0, :].sum())\n",
        "        # print(scores[0, 0])\n",
        "\n",
        "        output = torch.matmul(scores, v) #(N, n_head, T, out_dim)\n",
        "        # print(output.shape)\n",
        "        return output, scores\n",
        "\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, n_head, in_dim, out_dim):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        self.linear_q = torch.nn.Linear(in_features=in_dim, out_features=n_head*out_dim)\n",
        "        self.linear_k = torch.nn.Linear(in_features=in_dim, out_features=n_head*out_dim)\n",
        "        self.linear_v = torch.nn.Linear(in_features=in_dim, out_features=n_head*out_dim)\n",
        "\n",
        "        self.scaled_dot_production_attention = ScaledDotProductAttention()\n",
        "        self.linear = torch.nn.Linear(in_features=n_head*out_dim, out_features=out_dim)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "\n",
        "\n",
        "        batch_size, len_q, len_kv = q.shape[0], q.shape[1], k.shape[1]\n",
        "\n",
        "        q = self.linear_q(q).view(batch_size, len_q, self.n_head, self.out_dim) #(N, T, in_dim) --> (N, T, n_head * out_dim) --> (N, T, n_head, out_dim)\n",
        "        k = self.linear_k(k).view(batch_size, len_kv, self.n_head, self.out_dim)\n",
        "        v = self.linear_v(v).view(batch_size, len_kv, self.n_head, self.out_dim)\n",
        "\n",
        "\n",
        "        q = q.transpose(1, 2) #(N, T, n_head, out_dim) --> (N, n_head, T, out_dim)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "        # print(q.shape, k.shape, v.shape)\n",
        "\n",
        "        output, scores = self.scaled_dot_production_attention(q, k, v, mask=mask)\n",
        "\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, len_q, -1)\n",
        "        # print(output.shape)\n",
        "\n",
        "        output = self.linear(output) #(N, T, n_head * out_dim) --> (N, T, out_dim)\n",
        "        # print(output.shape)\n",
        "\n",
        "        return output, scores\n",
        "\n",
        "class PositionWiseFeedForward(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.linear_1 = torch.nn.Linear(in_features=in_dim, out_features=hidden_dim)\n",
        "        self.linear_2 = torch.nn.Linear(in_features=hidden_dim, out_features=in_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear_1(x)\n",
        "        x = torch.nn.ReLU()(x)\n",
        "        x = self.linear_2(x)\n",
        "        return x\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, n_head, in_dim, out_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.position_enc = PositionalEncoding(in_dim, out_dim)\n",
        "\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(n_head=n_head, in_dim=out_dim, out_dim=out_dim)\n",
        "        self.layer_norm_1_1 = torch.nn.LayerNorm(out_dim)\n",
        "\n",
        "        self.position_wise_feed_forward_1 = PositionWiseFeedForward(out_dim, hidden_dim=128)\n",
        "        self.layer_norm_1_2 = torch.nn.LayerNorm(out_dim)\n",
        "\n",
        "        self.scores_for_paint = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        qkv = self.position_enc(x) #(N, T, 37) --> (N, T, 64)\n",
        "\n",
        "        residual = qkv\n",
        "        outputs, scores = self.multi_head_attention_1(qkv, qkv, qkv)\n",
        "        self.scores_for_paint = scores.detach().cpu().numpy()\n",
        "        outputs = self.layer_norm_1_1(outputs + residual) #Add & Norm\n",
        "        # print(outputs.shape)\n",
        "\n",
        "        residual = outputs\n",
        "        outputs = self.position_wise_feed_forward_1(outputs)\n",
        "        outputs = self.layer_norm_1_2(outputs + residual) #Add & Norm\n",
        "        # print(outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "def get_subsequent_mask(seq):\n",
        "    seq_len = seq.shape[1]\n",
        "    ones = torch.ones((seq_len, seq_len), dtype=torch.int, device=seq.device)\n",
        "    mask = 1 - torch.triu(ones, diagonal=1)\n",
        "    # print(mask)\n",
        "    return mask\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, n_head, in_dim, out_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.position_enc = PositionalEncoding(in_dim, out_dim)\n",
        "\n",
        "        self.multi_head_attention_1_1 = MultiHeadAttention(n_head=n_head, in_dim=out_dim, out_dim=out_dim)\n",
        "        self.layer_norm_1_1 = torch.nn.LayerNorm(out_dim)\n",
        "\n",
        "        self.multi_head_attention_1_2 = MultiHeadAttention(n_head=n_head, in_dim=out_dim, out_dim=out_dim)\n",
        "        self.layer_norm_1_2 = torch.nn.LayerNorm(out_dim)\n",
        "\n",
        "        self.position_wise_feed_forward_1 = PositionWiseFeedForward(out_dim, hidden_dim=128)\n",
        "        self.layer_norm_1_3 = torch.nn.LayerNorm(out_dim)\n",
        "\n",
        "        self.scores_for_paint = None\n",
        "\n",
        "    def forward(self, enc_outputs, target):\n",
        "        qkv = self.position_enc(target)\n",
        "\n",
        "        residual = qkv\n",
        "        outputs, scores = self.multi_head_attention_1_1(qkv, qkv, qkv, mask=get_subsequent_mask(target))\n",
        "        outputs = self.layer_norm_1_1(outputs + residual)\n",
        "        # print(outputs.shape)\n",
        "\n",
        "        residual = outputs\n",
        "        outputs, scores = self.multi_head_attention_1_2(outputs, enc_outputs, enc_outputs)\n",
        "        self.scores_for_paint = scores.detach().cpu().numpy()\n",
        "        outputs = self.layer_norm_1_2(outputs + residual)\n",
        "\n",
        "        residual = outputs\n",
        "        outputs = self.position_wise_feed_forward_1(outputs)\n",
        "        outputs = self.layer_norm_1_3(outputs + residual)\n",
        "        # print(outputs.shape)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class Transformer(torch.nn.Module):\n",
        "    def __init__(self, n_head):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder(n_head, in_dim=37, out_dim=64)\n",
        "        self.decoder = Decoder(n_head, in_dim=12, out_dim=64)\n",
        "        self.linear = torch.nn.Linear(in_features=64, out_features=12)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        enc_outputs = self.encoder(x)\n",
        "        outputs = self.decoder(enc_outputs, y)\n",
        "        outputs = self.linear(outputs)\n",
        "        # print(outputs.shape)\n",
        "        outputs = torch.nn.Softmax(dim=-1)(outputs)\n",
        "        # print(outputs.shape)\n",
        "        return outputs\n",
        "\n",
        "    def size(self):\n",
        "        size = sum([p.numel() for p in self.parameters()])\n",
        "        print('%.2fKB' % (size * 4 / 1024))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = Transformer(n_head=4)\n",
        "    model.size()\n",
        "\n",
        "    batch_x = torch.randn(16, 10, 37) #(N, T, in_dim)\n",
        "    batch_y = torch.randn(16, 7, 12) #(N, T, in_dim)\n",
        "    # print(batch_x.shape)\n",
        "\n",
        "    pred = model(batch_x, batch_y)\n",
        "    print(pred.shape)\n",
        "    print(pred[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgcoP6AvuDAO"
      },
      "source": [
        "## save_load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FHZ3UI1JuGXY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def find_last_checkpoint(checkpoint_dir):\n",
        "    epochs = []\n",
        "    for name in os.listdir(checkpoint_dir):\n",
        "        if os.path.splitext(name)[-1] == '.pth':\n",
        "            epochs += [int(name.strip('ckpt_epoch_.pth'))]\n",
        "    if len(epochs) == 0:\n",
        "        raise IOError('no checkpoint found in {}'.format(checkpoint_dir))\n",
        "    return max(epochs)\n",
        "\n",
        "def save_checkpoint(checkpoint_dir, epoch, model, optimizer=None):\n",
        "    checkpoint = {}\n",
        "    checkpoint['epoch'] = epoch\n",
        "\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        model_state_dict = model.module.state_dict()\n",
        "    else:\n",
        "        model_state_dict = model.state_dict()\n",
        "    checkpoint['model'] = model_state_dict\n",
        "\n",
        "    if optimizer is not None:\n",
        "        optimizer_state_dict = optimizer.state_dict()\n",
        "        # for k, v in optimizer_state_dict.items():\n",
        "        #     print(k, type(v))\n",
        "        # optimizer_state_dict = rename_dict_key(optimizer_state_dict)\n",
        "        checkpoint['optimizer'] = optimizer_state_dict\n",
        "    else:\n",
        "        checkpoint['optimizer'] = None\n",
        "\n",
        "    torch.save(checkpoint, os.path.join(checkpoint_dir, 'ckpt_epoch_%02d.pth'% epoch))\n",
        "\n",
        "def load_checkpoint(checkpoint_dir, epoch=-1):\n",
        "    if epoch == -1:\n",
        "        epoch = find_last_checkpoint(checkpoint_dir)\n",
        "    checkpoint_name = 'ckpt_epoch_%02d.pth'% epoch\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_name)\n",
        "    ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
        "    return ckpt\n",
        "\n",
        "def save_model(checkpoint_dir, epoch, model):\n",
        "    save_checkpoint(checkpoint_dir, epoch, model, optimizer=None)\n",
        "\n",
        "def load_model(checkpoint_dir, epoch, model):\n",
        "    try:\n",
        "        ckpt = load_checkpoint(checkpoint_dir, epoch)\n",
        "        model_state_dict = ckpt['model']\n",
        "\n",
        "        if isinstance(model, torch.nn.DataParallel):\n",
        "            model.module.load_state_dict(model_state_dict)\n",
        "        # elif isinstance(model, torchDDP):\n",
        "        #     model.module.load_state_dict(model_state_dict)\n",
        "        # elif isinstance(model, apexDDP):\n",
        "        #     model.module.load_state_dict(model_state_dict)\n",
        "        else:\n",
        "            model.load_state_dict(model_state_dict)\n",
        "    except Exception as e:\n",
        "        print('failed to load model, {}'.format(e))\n",
        "    return model\n",
        "\n",
        "def load_optimizer(checkpoint_dir, epoch, optimizer):\n",
        "    try:\n",
        "        ckpt = load_checkpoint(checkpoint_dir, epoch)\n",
        "        optimizer_state_dict = ckpt['optimizer']\n",
        "        optimizer.load_state_dict(optimizer_state_dict)\n",
        "    except Exception as e:\n",
        "        print('failed to load optimizer, {}'.format(e))\n",
        "    return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EAR8fFvhVZjD",
        "outputId": "68ea7aab-2dca-4251-cda2-64f839399733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-25.8.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.8 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Installing collected packages: faker\n",
            "Successfully installed faker-25.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install faker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eB0xyPIUUmRr",
        "outputId": "fcb5a6d0-b3ce-4cfa-972c-ac1456abe650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            " 26%|██▌       | 2593/10000 [00:00<00:00, 25928.42it/s]\u001b[A\n",
            " 52%|█████▏    | 5186/10000 [00:00<00:00, 24034.07it/s]\u001b[A\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 25158.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "use gpu\n",
            "train form epoch 20\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "from dataset import Dataset, transform, collate_fn\n",
        "from model import Transformer\n",
        "import save_load as sl\n",
        "\n",
        "def calc_accuracy(pred, answer):\n",
        "    # print(pred)\n",
        "    pred = np.argmax(pred, axis=2)\n",
        "    answer = np.argmax(answer, axis=2)\n",
        "    # print(pred.shape, answer.shape)\n",
        "    correct = (pred == answer).astype(np.int_)\n",
        "    accuracy = correct.sum() / (pred.shape[0] * pred.shape[1])\n",
        "    # print(accuracy)\n",
        "    return accuracy\n",
        "\n",
        "def train(model, loss_fn, optimizer, dataloader, epoch, use_gpu=False):\n",
        "    pbar = tqdm(total=len(dataloader), bar_format='{l_bar}{r_bar}', dynamic_ncols=True)\n",
        "    pbar.set_description(f'Epoch %d' % epoch)\n",
        "\n",
        "    for step, (batch_x, batch_y, _) in enumerate(dataloader):\n",
        "        if use_gpu:\n",
        "            batch_x = batch_x.cuda()\n",
        "            batch_y = batch_y.cuda()\n",
        "        pred = model(batch_x, batch_y[:, :-1, :])\n",
        "        accuracy = calc_accuracy(pred.detach().cpu().numpy(), batch_y[:, 1:, :].detach().cpu().numpy())\n",
        "        # loss = loss_fn(pred.transpose(1, 2), batch_y)\n",
        "        loss = loss_fn(pred, batch_y[:, 1:, :])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_postfix(**{'loss':loss.detach().cpu().item(), 'accuracy':accuracy})\n",
        "        pbar.update()\n",
        "    sl.save_checkpoint('/content/sample_data/checkpoint', epoch, model, optimizer)\n",
        "\n",
        "    pbar.close()\n",
        "\n",
        "def main(gpu_id=None):\n",
        "    dataset = Dataset(transform=transform, n_datas=10000)\n",
        "    pad_vec = np.zeros(len(dataset.human_vocab))\n",
        "    pad_vec[dataset.human_vocab['<pad>']] = 1\n",
        "    dataloader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                            batch_size=6,\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=6,\n",
        "                                            collate_fn=partial(collate_fn, pad_vec))\n",
        "\n",
        "    model = Transformer(n_head=2)\n",
        "    if gpu_id is not None:\n",
        "        print('use gpu')\n",
        "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_id\n",
        "        n_gpus = torch.cuda.device_count()\n",
        "        # print('use %d gpu [%s]' % (n_gpus, gpu_id))\n",
        "        model = model.cuda()\n",
        "        # model = torch.nn.DataParallel(model, device_ids=[i for i in range(n_gpus)])\n",
        "    # loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    model = sl.load_model('/content/sample_data/checkpoint', -1, model)\n",
        "    optimizer = sl.load_optimizer('/content/sample_data/checkpoint', -1, optimizer)\n",
        "\n",
        "    try:\n",
        "        trained_epoch = sl.find_last_checkpoint('/content/sample_data/checkpoint')\n",
        "        print('train form epoch %d' % (trained_epoch + 1))\n",
        "    except Exception as e:\n",
        "        print('train from the very begining, {}'.format(e))\n",
        "        trained_epoch = -1\n",
        "    for epoch in range(trained_epoch+1, 5):\n",
        "        train(model, loss_fn, optimizer, dataloader, epoch, use_gpu=True if gpu_id is not None else False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    if len(sys.argv) == 1:\n",
        "        main(gpu_id=None)\n",
        "    else:\n",
        "        main(gpu_id='0')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "OUJuE-5dTU5B"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        },
        "id": "NOiZPAIEpb4x",
        "outputId": "ad613bdb-821e-4718-b814-9821da8cb8dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\n",
            " 27%|██▋       | 2665/10000 [00:00<00:00, 26641.96it/s]\u001b[A\n",
            " 53%|█████▎    | 5330/10000 [00:00<00:00, 26396.81it/s]\u001b[A\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 25785.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load model 19\n",
            "[8/22/81] --> [1982-08-22<pad><pad><pad><pad>], answer: [['1', '9', '8', '1', '-', '0', '8', '-', '2', '2']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 0: 100%|| 1667/1667 [11:16<00:00,  2.46it/s, accuracy=0.977, loss=0.00196]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x850 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def translate(model, x, y0):\n",
        "    x_tensor = torch.from_numpy(x).unsqueeze(0)\n",
        "    enc_output = model.encoder(x_tensor)\n",
        "    y0_tensor = torch.from_numpy(y0).unsqueeze(0).unsqueeze(0)\n",
        "    y_tensor = y0_tensor.clone()\n",
        "\n",
        "    for step in range(14):\n",
        "        dec_output = model.decoder(enc_output, y_tensor)\n",
        "        pred = model.linear(dec_output)\n",
        "        pred = torch.nn.Softmax(dim=-1)(pred)\n",
        "\n",
        "        y_tensor = torch.cat((y0_tensor, pred), dim=1)\n",
        "        # print(y_tensor.shape)\n",
        "    return y_tensor.squeeze(0)\n",
        "\n",
        "def paint_score(score, human_readable, pred):\n",
        "    '''\n",
        "    score: (n_head, T_dec, T_enc)\n",
        "    human_readabe: string, length is T_enc\n",
        "    pred: string, length is T_dec\n",
        "    '''\n",
        "    # print(score.shape, len(pred), len(human_readable))\n",
        "    # print(pred, human_readable)\n",
        "\n",
        "    n_head = score.shape[0]\n",
        "\n",
        "    f = plt.figure(figsize=(8, 8.5))\n",
        "    for i in range(n_head):\n",
        "        ax = f.add_subplot(n_head, 1, i+1)\n",
        "        i = ax.imshow(score[i, :10, :], interpolation='nearest', cmap='Blues')\n",
        "\n",
        "        ax.set_xticks(range(min(30, len(human_readable))))\n",
        "        ax.set_xticklabels(human_readable[:30], rotation=0)\n",
        "\n",
        "        #ax.set_xticks(range(30))\n",
        "        #ax.set_xticklabels(human_readable[:30], rotation=0)\n",
        "\n",
        "        ax.set_yticks(range(10))\n",
        "        ax.set_yticklabels(pred[:10], rotation=0)\n",
        "\n",
        "    plt.savefig('./attention.png')\n",
        "\n",
        "def main():\n",
        "    dataset = Dataset(transform=transform, n_datas=10000, seed=None)\n",
        "    model = Transformer(n_head=2)\n",
        "    try:\n",
        "        trained_epoch = sl.find_last_checkpoint('/content/sample_data/checkpoint')\n",
        "        print('load model %d' % (trained_epoch))\n",
        "    except Exception as e:\n",
        "        print('no trained model found, {}'.format(e))\n",
        "        return\n",
        "    model = sl.load_model('/content/sample_data/checkpoint', -1, model)\n",
        "    model.eval()\n",
        "\n",
        "    x, y, extra = dataset.__getitem__(0)\n",
        "    # print(x.shape, y.shape)\n",
        "    # pred = model(torch.from_numpy(x).unsqueeze(0), torch.from_numpy(y).unsqueeze(0)).squeeze()\n",
        "    pred = translate(model, x, y[0])\n",
        "    # print(pred.shape)\n",
        "    pred = np.argmax(pred.detach().numpy(), axis=1)[1:]\n",
        "    # print(extra['machine_readable'])\n",
        "    pred = [dataset.inv_machine_vocab[p] for p in pred]\n",
        "    pred_str = ''.join(pred)\n",
        "    human_readable = extra['human_readable']\n",
        "    machine_readable = extra['machine_readable']\n",
        "    print('[%s] --> [%s], answer: [%s]' % (human_readable, pred_str, list(machine_readable)))\n",
        "\n",
        "    dec_scores = model.decoder.scores_for_paint\n",
        "    # print(dec_scores.shape)\n",
        "    paint_score(dec_scores[0], human_readable, pred)\n",
        "\n",
        "    # print(len(model.scores_for_paint), model.scores_for_paint[0].shape)\n",
        "    # scores = np.array(model.scores_for_paint)\n",
        "    # print(np.argmax(scores, axis=1))\n",
        "\n",
        "    # f = plt.figure(figsize=(8, 8.5))\n",
        "    # ax = f.add_subplot(1, 1, 1)\n",
        "    # i = ax.imshow(scores, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "    # ax.set_xticks(range(30))\n",
        "    # ax.set_xticklabels(human_readable[:30], rotation=0)\n",
        "\n",
        "    # ax.set_yticks(range(10))\n",
        "    # ax.set_yticklabels(machine_readable[:10], rotation=0)\n",
        "\n",
        "    # plt.savefig('./attention.png')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtyhJ05AVZcT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}