{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCHY8bSXE/2PpeIwUR9j1N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bindulekh/private/blob/main/prompt_engineering_practice_notebook_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt Engineering Best Practices for Instruction-Tuned LLM"
      ],
      "metadata": {
        "id": "K-uu63-5cRQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you ever wondered why your interaction with a language model falls short of expectations? The answer may lie in the clarity of your instructions.\n",
        "\n",
        "Picture this scenario: requesting someone, perhaps a bright but task-unaware individual, to write about a popular figure. It’s not just about the subject; clarity extends to specifying the focus — scientific work, personal life, historical role — and even the desired tone, be it professional or casual. Much like guiding a fresh graduate through the task, offering specific snippets for preparation sets the stage for success.\n",
        "\n",
        "In this notebook, we’re going to help you make your talks with the language model better by getting really good at giving clear and specific instructions to get the expected output."
      ],
      "metadata": {
        "id": "yVvde8dacV8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Setting Up Work Environment"
      ],
      "metadata": {
        "id": "z_aEWlo7cbLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-generativeai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDiVug-I0PQB",
        "outputId": "78a3fbb0-28bc-49d8-eacc-648158365054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.5.4)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.4)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.16.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.84.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.4->google-generativeai) (1.24.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.63.2)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.31.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.1.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.20.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1dev,>=0.15.0->google-api-python-client->google-generativeai) (3.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.6.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "hEr1YlDC0emj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = 'Google API Key'\n",
        "genai.configure(api_key=api_key)"
      ],
      "metadata": {
        "id": "YGt6ByPX0zdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = {\n",
        "  \"temperature\": 1,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash\",\n",
        "  generation_config=generation_config,\n",
        "  # safety_settings = Adjust safety settings\n",
        "  # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
        ")\n",
        "prompt = f\"\"\"\n",
        "Summarize the text into a single sentence.\"\"\"\n",
        "\n",
        "contents = ['''\n",
        "You should express what you want a model to do by \\\n",
        "providing instructions that are as clear and \\\n",
        "specific as you can possibly make them. \\\n",
        "This will guide the model towards the desired output, \\\n",
        "and reduce the chances of receiving irrelevant \\\n",
        "or incorrect responses. Don't confuse writing a \\\n",
        "clear prompt with writing a short prompt. \\\n",
        "In many cases, longer prompts provide more clarity \\\n",
        "and context for the model, which can lead to \\\n",
        "more detailed and relevant outputs.\n",
        "''', prompt]\n",
        "\n",
        "response = model.generate_content(contents)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "3Obz85pU0m3y",
        "outputId": "d3106bb1-14c4-4c86-e2dd-18a0268e1fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You should provide clear and specific instructions to guide the model towards the desired output, using longer prompts when necessary for clarity and context. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2. Ask for a structured output\n",
        "The next tactic is to ask for a structured output. To make parsing the model outputs easier, it can be helpful to ask for a structured output like HTML or JSON.\n",
        "\n",
        "So in the prompt, we’re saying generate a list of three made-up book titles along with their authors and genres. Provide them in JSON format with the following keys, book ID, title, author, and genre."
      ],
      "metadata": {
        "id": "xYPtLve8OwGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash',\n",
        "                              generation_config={\"response_mime_type\": \"application/json\"})\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Generate a list of three made-up book titles along \\\n",
        "with their authors and genres.\n",
        "Provide them in JSON format with the following keys:\n",
        "book_id, title, author, genre.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "8iKFHBqfOqA-",
        "outputId": "1f95f051-5e11-4afe-dbee-fce5186b998b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"books\": [{\"book_id\": \"1\", \"title\": \"Whispers of the Ancient Forest\", \"author\": \"Elara Blackwood\", \"genre\": \"Fantasy\"}, {\"book_id\": \"2\", \"title\": \"The Clockwork Detective\", \"author\": \"Jasper Thorne\", \"genre\": \"Steampunk\"}, {\"book_id\": \"3\", \"title\": \"The Last Stargazer\", \"author\": \"Anya Petrova\", \"genre\": \"Science Fiction\"}]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3. Ask the model to check whether conditions are satisfied\n",
        "The next tactic is to ask the model to check whether conditions are satisfied. If the task makes assumptions that aren’t necessarily satisfied, then we can tell the model to check these assumptions first. Then if they’re not satisfied, indicate this and kind of stop short of a full task completion attempt. You might also consider potential edge cases and how the model should handle them to avoid unexpected errors or results.\n",
        "\n",
        "Let's take a paragraph describing the steps to make a cup of tea. And then I will copy over the prompt. So the prompt you’ll be provided with text delimited by triple quotes. If it contains a sequence of instructions, rewrite those instructions in the following format and then just the steps written out. If the text does not contain a sequence of instructions, then simply write, no steps provided."
      ],
      "metadata": {
        "id": "hrXF659vREJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash\",\n",
        "  generation_config={\"response_mime_type\" : \"text/plain\"}\n",
        "  # safety_settings = Adjust safety settings\n",
        "  # See https://ai.google.dev/gemini-api/docs/safety-settings\n",
        ")\n",
        "text_1 = f\"\"\"\n",
        "Making a cup of tea is easy! First, you need to get some \\\n",
        "water boiling. While that's happening, \\\n",
        "grab a cup and put a tea bag in it. Once the water is \\\n",
        "hot enough, just pour it over the tea bag. \\\n",
        "Let it sit for a bit so the tea can steep. After a \\\n",
        "few minutes, take out the tea bag. If you \\\n",
        "like, you can add some sugar or milk to taste. \\\n",
        "And that's it! You've got yourself a delicious \\\n",
        "cup of tea to enjoy.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "You will be provided with text delimited by triple quotes.\n",
        "If it contains a sequence of instructions, \\\n",
        "re-write those instructions in the following format:\n",
        "\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\\n",
        "then simply write \\\"No steps provided.\\\"\n",
        "\n",
        "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = model.generate_content(prompt)\n",
        "print(\"Completion for Text 1:\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "WW4JasBGRawa",
        "outputId": "f053dea2-3360-48d3-91ee-08080d766b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion for Text 1:\n",
            "Step 1 - Get some water boiling.\n",
            "Step 2 - Grab a cup and put a tea bag in it.\n",
            "Step 3 - Pour the boiling water over the tea bag.\n",
            "Step 4 - Let the tea steep for a few minutes.\n",
            "Step 5 - Take out the tea bag.\n",
            "Step 6 - Add sugar or milk to taste (optional). \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s try this same prompt with a different paragraph. This paragraph is just describing a sunny day, it doesn’t have any instructions in it. So, if we take the same prompt we used earlier and instead run it on this text, the model will try and extract the instructions. If it doesn’t find any, we’re going to ask it to just say, no steps provided.\n",
        "\n"
      ],
      "metadata": {
        "id": "2M5cLWZVVj_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = f\"\"\"\n",
        "The sun is shining brightly today, and the birds are \\\n",
        "singing. It's a beautiful day to go for a \\\n",
        "walk in the park. The flowers are blooming, and the \\\n",
        "trees are swaying gently in the breeze. People \\\n",
        "are out and about, enjoying the lovely weather. \\\n",
        "Some are having picnics, while others are playing \\\n",
        "games or simply relaxing on the grass. It's a \\\n",
        "perfect day to spend time outdoors and appreciate the \\\n",
        "beauty of nature.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "You will be provided with text delimited by triple quotes.\n",
        "If it contains a sequence of instructions, \\\n",
        "re-write those instructions in the following format:\n",
        "\n",
        "Step 1 - ...\n",
        "Step 2 - …\n",
        "…\n",
        "Step N - …\n",
        "\n",
        "If the text does not contain a sequence of instructions, \\\n",
        "then simply write \\\"No steps provided.\\\"\n",
        "\n",
        "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "response = model.generate_content(prompt)\n",
        "print(\"Completion for Text 2:\")\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "J5JfpNprVrwI",
        "outputId": "daa1e8dc-8ab7-413f-cc53-c5981ae8988b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion for Text 2:\n",
            "No steps provided. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4. Few-shot prompting\n",
        "The final tactic for this principle is what we call few-shot prompting. This is just providing examples of successful executions of the task you want to be performed before asking the model to do the actual task you want it to do.\n",
        "\n",
        "In this prompt, we’re telling the model that its task is to answer in a consistent style. We have this example of a kind of conversation between a child and a grandparent.\n",
        "\n",
        "The kind of child who says, teach me about patience. The grandparent responds with these kinds of metaphors. And so, since we’ve kind of told the model to answer in a consistent tone, now we’ve said, teach me about resilience.\n",
        "\n",
        "Since the model kind of has this few-shot example, it will respond in a similar tone to this next instruction. So, resilience is like a tree that bends with the wind but never breaks, and so on."
      ],
      "metadata": {
        "id": "3Y-VLmeoWA7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to answer in a consistent style.\n",
        "\n",
        "<child>: Teach me about patience.\n",
        "\n",
        "<grandparent>: The river that carves the deepest \\\n",
        "valley flows from a modest spring; the \\\n",
        "grandest symphony originates from a single note; \\\n",
        "the most intricate tapestry begins with a solitary thread.\n",
        "\n",
        "<child>: Teach me about resilience.\n",
        "\"\"\"\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "3NigucW1WCvF",
        "outputId": "08cd1f0c-0294-4b6a-e62d-054769f8f5da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<grandparent>: The willow bends low in the wind, but its roots hold firm. The seed buried deep in the earth endures the winter's chill to blossom in spring. The eagle soars above the storm, its wings unyielding in the face of adversity. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Give the model time to think\n",
        "The second principle is to give the model time to think. Suppose a model is making reasoning errors by rushing to an incorrect conclusion. In that case, you should try reframing the query to request a chain or series of relevant reasoning before the model provides its final answer.\n",
        "\n",
        "Another way to think about this is that if you give a model a task that’s too complex for it to do in a short amount of time or a small number of words, it may make up a guess that is likely to be incorrect and this would happen to a person too.\n",
        "\n",
        "If you ask someone to complete a complex math question without time to work out the answer first, they would also likely make a mistake. So, in these situations, you can instruct the model to think longer about a problem, which means it’s spending more computational effort on the task. Let’s go over some tactics for the second principle."
      ],
      "metadata": {
        "id": "lzoXxWXJWey-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Specify the steps required to complete a task\n",
        "The first tactic is to specify the steps required to complete a task. Let's take for example the given prompt which is a description of the story of Jack and Jill. In this prompt, the instructions are to perform the following actions. First, summarize the following text delimited by triple backticks with one sentence. Second, translate the summary into French. Third, list each name in the French summary. Fourth, output a JSON object that contains the following keys, French summary, and num names. And then we want it to separate the answers with line breaks. And so, we add the text, which is just this paragraph."
      ],
      "metadata": {
        "id": "HENIqAT6Wicr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "In a charming village, siblings Jack and Jill set out on \\\n",
        "a quest to fetch water from a hilltop \\\n",
        "well. As they climbed, singing joyfully, misfortune \\\n",
        "struck—Jack tripped on a stone and tumbled \\\n",
        "down the hill, with Jill following suit. \\\n",
        "Though slightly battered, the pair returned home to \\\n",
        "comforting embraces. Despite the mishap, \\\n",
        "their adventurous spirits remained undimmed, and they \\\n",
        "continued exploring with delight.\n",
        "\"\"\"\n",
        "# example 1\n",
        "prompt_1 = f\"\"\"\n",
        "Perform the following actions:\n",
        "1 - Summarize the following text delimited by triple \\\n",
        "backticks with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the following \\\n",
        "keys: french_summary, num_names.\n",
        "\n",
        "Separate your answers with line breaks.\n",
        "Text:\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = model.generate_content(prompt_1)\n",
        "print(\"Completion for prompt 1:\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "A44JPk9ZWmZV",
        "outputId": "009a660e-5209-464a-e59f-bbb17840904a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completion for prompt 1:\n",
            "Jack and Jill, siblings from a charming village, had a mishap while fetching water but remained undeterred in their adventurous spirit.\n",
            "\n",
            "Jack et Jill, frère et sœur d'un charmant village, ont eu un accident en allant chercher de l'eau mais sont restés sans peur dans leur esprit aventureux.\n",
            "\n",
            "- Jack\n",
            "- Jill\n",
            "\n",
            "```json\n",
            "{\n",
            "\"french_summary\": \"Jack et Jill, frère et sœur d'un charmant village, ont eu un accident en allant chercher de l'eau mais sont restés sans peur dans leur esprit aventureux.\",\n",
            "\"num_names\": 2\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we run this you can see that we have the summarized text. Then we have the French translation. Finally, we have the names. It gave the names a title in French. Then we have the JSON that we requested. Let's take another prompt to complete the same task. In this prompt, we will use a format that specifies the output structure for the model because as you notice in this example, this name’s title is in French which we might not necessarily want.\n",
        "\n",
        "In this prompt, we’re asking for something similar. The beginning of the prompt is the same, we’re just asking for the same steps and then we’re asking the model to use the following format. So we have specified the exact format of text, summary, translation, names, and output JSON. Finally, we start by just saying the text to summarize or we can even just say the text."
      ],
      "metadata": {
        "id": "3IRsOm8RW_G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_2 = f\"\"\"\n",
        "Your task is to perform the following actions:\n",
        "1 - Summarize the following text delimited by\n",
        "  <> with 1 sentence.\n",
        "2 - Translate the summary into French.\n",
        "3 - List each name in the French summary.\n",
        "4 - Output a json object that contains the\n",
        "  following keys: french_summary, num_names.\n",
        "\n",
        "Use the following format:\n",
        "Text: <text to summarize>\n",
        "Summary: <summary>\n",
        "Translation: <summary translation>\n",
        "Names: <list of names in summary>\n",
        "Output JSON: <json with summary and num_names>\n",
        "Text: <{text}>\n",
        "\"\"\"\n",
        "response = model.generate_content(prompt_2)\n",
        "print(\"\\nCompletion for prompt 2:\")\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "P4GSmYhzXLiz",
        "outputId": "23e5133d-aa26-4c68-e1a2-530c4fd87f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Completion for prompt 2:\n",
            "Text: <\n",
            "In a charming village, siblings Jack and Jill set out on a quest to fetch water from a hilltop well. As they climbed, singing joyfully, misfortune struck—Jack tripped on a stone and tumbled down the hill, with Jill following suit. Though slightly battered, the pair returned home to comforting embraces. Despite the mishap, their adventurous spirits remained undimmed, and they continued exploring with delight.\n",
            ">\n",
            "\n",
            "Summary: Jack and Jill, siblings from a charming village, embark on a water-fetching adventure that ends in a comical tumble down a hill, but their spirits remain unbroken. \n",
            "\n",
            "Translation: Jack et Jill, frère et sœur d'un village charmant, partent à l'aventure pour aller chercher de l'eau à un puits situé au sommet d'une colline, mais leur périple se termine par une chute comique en bas de la colline, sans pour autant entamer leur enthousiasme.\n",
            "\n",
            "Names: Jack, Jill \n",
            "\n",
            "Output JSON: \n",
            "```json\n",
            "{\n",
            "  \"french_summary\": \"Jack et Jill, frère et sœur d'un village charmant, partent à l'aventure pour aller chercher de l'eau à un puits situé au sommet d'une colline, mais leur périple se termine par une chute comique en bas de la colline, sans pour autant entamer leur enthousiasme.\",\n",
            "  \"num_names\": 2\n",
            "}\n",
            "``` \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see, that this is the completion and the model has used the format that we asked for. So, we already gave it the text, and then it gave us the summary, the translation, the names, and the output JSON. This is sometimes nice because it’s going to be easier to pass this with code because it kind of has a more standardized format that you can kind of predict.\n",
        "\n",
        "Also notice that in this case, we’ve used angled brackets as the delimiter instead of triple backticks. You can choose any delimiters that make sense to you, and that make sense to the model."
      ],
      "metadata": {
        "id": "lEVB_z6uXVuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2. Instruct the model to work out its solution before rushing to a conclusion\n",
        "The next tactic is to instruct the model to work out its own solution before rushing to a conclusion. Sometimes we get better results when we explicitly instruct the models to reason out its own solution before concluding.\n",
        "\n",
        "This is the same idea that we were discussing before which is giving the model time to work things out before just kind of saying if an answer is correct or not, in the same way that a person would.\n",
        "\n",
        "In this prompt, we’re asking the model to determine if the student’s solution is correct or not. We have this math question first, and then we have the student’s solution. The student’s solution is incorrect because he has calculated the maintenance cost to be 100,000 plus 100x, but actually, it should be 10x, because it’s only $10 per square foot, where x is the kind of size of the insulation in square feet, as they’ve defined it. This should actually be 360x plus 100,000, not 450x. If we run this code, the model says the student’s solution is correct."
      ],
      "metadata": {
        "id": "c_JkPXs8XbFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Determine if the student's solution is correct or not.\n",
        "\n",
        "Question:\n",
        "I'm building a solar power installation and I need \\\n",
        " help working out the financials.\n",
        "- Land costs $100 / square foot\n",
        "- I can buy solar panels for $250 / square foot\n",
        "- I negotiated a contract for maintenance that will cost \\\n",
        "me a flat $100k per year, and an additional $10 / square \\\n",
        "foot\n",
        "What is the total cost for the first year of operations\n",
        "as a function of the number of square feet.\n",
        "\n",
        "Student's Solution:\n",
        "Let x be the size of the installation in square feet.\n",
        "Costs:\n",
        "1. Land cost: 100x\n",
        "2. Solar panel cost: 250x\n",
        "3. Maintenance cost: 100,000 + 100x\n",
        "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
        "\"\"\"\n",
        "response = model.generate_content(prompt)\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "Ei8YLVPIXgOM",
        "outputId": "bece8542-397d-46eb-dba9-9a6551ea8290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The student's solution is **almost correct**, but there's a small error in the maintenance cost calculation.\n",
            "\n",
            "Here's the breakdown:\n",
            "\n",
            "* **Land cost:** 100x (correct)\n",
            "* **Solar panel cost:** 250x (correct)\n",
            "* **Maintenance cost:** 100,000 + **10x** (**not** 100x). The student incorrectly used 100 instead of 10 for the per-square-foot maintenance cost.\n",
            "\n",
            "**Corrected Total cost:** 100x + 250x + 100,000 + 10x = **360x + 100,000** \n",
            "\n",
            "**Therefore, the correct total cost for the first year of operations as a function of the number of square feet is 360x + 100,000.** \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model agreed with the student's answer because it just kind of skim-read it. We can fix this by instructing the model to work out its solution first, and then compare its solution to the student’s solution.\n",
        "\n",
        "We can do it by the prompt below. This prompt is a lot longer. In this prompt, we inform the model that your task is to determine if the student’s solution is correct or not. First, work out your own solution to the problem. Then, compare your solution to the student’s solution and evaluate if the student’s solution is correct or not.\n",
        "\n",
        "Don’t decide if the student’s solution is correct until you have done the problem yourself. We have used the same trick to use the following format. The format will be the question, the student’s solution, the actual solution, and then whether the solution agrees, yes or no, and then the student's grade, correct or incorrect. Let's run the following prompt and see the answer by the model."
      ],
      "metadata": {
        "id": "4SQ8509lX2I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Your task is to determine if the student's solution \\\n",
        "is correct or not.\n",
        "To solve the problem do the following:\n",
        "- First, work out your own solution to the problem including the final total.\n",
        "- Then compare your solution to the student's solution \\\n",
        "and evaluate if the student's solution is correct or not.\n",
        "Don't decide if the student's solution is correct until\n",
        "you have done the problem yourself.\n",
        "\n",
        "Use the following format:\n",
        "Question:\n",
        "```\n",
        "question here\n",
        "```\n",
        "Student's solution:\n",
        "```\n",
        "student's solution here\n",
        "```\n",
        "Actual solution:\n",
        "```\n",
        "steps to work out the solution and your solution here\n",
        "```\n",
        "Is the student's solution the same as actual solution \\\n",
        "just calculated:\n",
        "```\n",
        "yes or no\n",
        "```\n",
        "Student grade:\n",
        "```\n",
        "correct or incorrect\n",
        "```\n",
        "\n",
        "Question:\n",
        "```\n",
        "I'm building a solar power installation and I need help \\\n",
        "working out the financials.\n",
        "- Land costs $100 / square foot\n",
        "- I can buy solar panels for $250 / square foot\n",
        "- I negotiated a contract for maintenance that will cost \\\n",
        "me a flat $100k per year, and an additional $10 / square \\\n",
        "foot\n",
        "What is the total cost for the first year of operations \\\n",
        "as a function of the number of square feet.\n",
        "```\n",
        "Student's solution:\n",
        "```\n",
        "Let x be the size of the installation in square feet.\n",
        "Costs:\n",
        "1. Land cost: 100x\n",
        "2. Solar panel cost: 250x\n",
        "3. Maintenance cost: 100,000 + 100x\n",
        "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
        "```\n",
        "Actual solution:\n",
        "\"\"\"\n",
        "response = model.generate_content(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "XwTuHhRhX6Ms",
        "outputId": "1b147bd2-724e-4609-c10e-55dadc4e6078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "response:\n",
            "GenerateContentResponse(\n",
            "    done=True,\n",
            "    iterator=None,\n",
            "    result=glm.GenerateContentResponse({\n",
            "      \"candidates\": [\n",
            "        {\n",
            "          \"finish_reason\": 4,\n",
            "          \"index\": 0,\n",
            "          \"safety_ratings\": [],\n",
            "          \"token_count\": 0,\n",
            "          \"grounding_attributions\": []\n",
            "        }\n",
            "      ],\n",
            "      \"usage_metadata\": {\n",
            "        \"prompt_token_count\": 399,\n",
            "        \"total_token_count\": 399,\n",
            "        \"candidates_token_count\": 0\n",
            "      }\n",
            "    }),\n",
            ")\n"
          ]
        }
      ]
    }
  ]
}